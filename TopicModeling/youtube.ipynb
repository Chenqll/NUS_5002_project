{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Let's not forget that Apple Pay in 2014 requir...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Here in NZ 50% of retailers don’t even have co...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>I will forever acknowledge this channel with t...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Whenever I go to a place that doesn’t take App...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Apple Pay is so convenient, secure, and easy t...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Video ID                                            Comment  \\\n",
       "0           0  wAZZ-UWGVHI  Let's not forget that Apple Pay in 2014 requir...   \n",
       "1           1  wAZZ-UWGVHI  Here in NZ 50% of retailers don’t even have co...   \n",
       "2           2  wAZZ-UWGVHI  I will forever acknowledge this channel with t...   \n",
       "3           3  wAZZ-UWGVHI  Whenever I go to a place that doesn’t take App...   \n",
       "4           4  wAZZ-UWGVHI  Apple Pay is so convenient, secure, and easy t...   \n",
       "\n",
       "   Likes  Sentiment  \n",
       "0   95.0        1.0  \n",
       "1   19.0        0.0  \n",
       "2  161.0        2.0  \n",
       "3    8.0        0.0  \n",
       "4   34.0        2.0  "
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments=pd.read_csv('data/comments.csv')\n",
    "stats=pd.read_csv('data/videos-stats.csv')\n",
    "comments.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete invalid col\n",
    "comments=comments.drop(columns=['Unnamed: 0'],axis=1)\n",
    "stats=stats.drop(columns=['Unnamed: 0'],axis=1)\n",
    "\n",
    "# delete nan value\n",
    "stats=stats.dropna()\n",
    "comments=comments.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Views</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Pay Is Killing the Physical Wallet After...</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>tech</td>\n",
       "      <td>3407.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>135612.0</td>\n",
       "      <td>137037.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The most EXPENSIVE thing I own.</td>\n",
       "      <td>b3x28s61q3c</td>\n",
       "      <td>2022-08-24</td>\n",
       "      <td>tech</td>\n",
       "      <td>76779.0</td>\n",
       "      <td>4306.0</td>\n",
       "      <td>1758063.0</td>\n",
       "      <td>1783680.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My New House Gaming Setup is SICK!</td>\n",
       "      <td>4mgePWWCAmA</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>tech</td>\n",
       "      <td>63825.0</td>\n",
       "      <td>3338.0</td>\n",
       "      <td>1564007.0</td>\n",
       "      <td>1585157.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petrol Vs Liquid Nitrogen | Freezing Experimen...</td>\n",
       "      <td>kXiYSI7H2b0</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>tech</td>\n",
       "      <td>71566.0</td>\n",
       "      <td>1426.0</td>\n",
       "      <td>922918.0</td>\n",
       "      <td>945243.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best Back to School Tech 2022!</td>\n",
       "      <td>ErMwWXQxHp0</td>\n",
       "      <td>2022-08-08</td>\n",
       "      <td>tech</td>\n",
       "      <td>96513.0</td>\n",
       "      <td>5155.0</td>\n",
       "      <td>1855644.0</td>\n",
       "      <td>1887690.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title     Video ID  \\\n",
       "0  Apple Pay Is Killing the Physical Wallet After...  wAZZ-UWGVHI   \n",
       "1                    The most EXPENSIVE thing I own.  b3x28s61q3c   \n",
       "2                 My New House Gaming Setup is SICK!  4mgePWWCAmA   \n",
       "3  Petrol Vs Liquid Nitrogen | Freezing Experimen...  kXiYSI7H2b0   \n",
       "4                     Best Back to School Tech 2022!  ErMwWXQxHp0   \n",
       "\n",
       "  Published At Keyword    Likes  Comments      Views        pop  \n",
       "0   2022-08-23    tech   3407.0     672.0   135612.0   137037.3  \n",
       "1   2022-08-24    tech  76779.0    4306.0  1758063.0  1783680.3  \n",
       "2   2022-08-23    tech  63825.0    3338.0  1564007.0  1585157.3  \n",
       "3   2022-08-23    tech  71566.0    1426.0   922918.0   945243.4  \n",
       "4   2022-08-08    tech  96513.0    5155.0  1855644.0  1887690.9  "
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caculate pop \n",
    "stats[\"pop\"]=stats.loc[:,[\"Likes\",\"Comments\",\"Views\"]].apply(lambda x:x[\"Likes\"]*0.3+x[\"Comments\"]*0.6+x['Views'],axis=1)\n",
    "\n",
    "stats.head()\n",
    "# comments likes views scale diff,so they need to be normalizeed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 下面生成的表 可以用作**时序分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nomalization\n",
    "# 在Python的sklearn包中提供了StandardScaler 和MinMaxScaler 可以很方便的对数据进行规范化处理\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing as sp\n",
    "\n",
    "video_likes=stats.loc[:,'Likes']\n",
    "video_comments=stats.loc[:,'Comments']\n",
    "video_views=stats.loc[:,'Views']\n",
    "\n",
    "standard = sp.MinMaxScaler()\n",
    "stats.loc[:,'Likes'] = standard.fit_transform(video_likes.values.reshape(-1,1))\n",
    "stats.loc[:,'Comments']=standard.fit_transform(video_comments.values.reshape(-1,1))\n",
    "stats.loc[:,'Views']=standard.fit_transform(video_views.values.reshape(-1,1))\n",
    "\n",
    "stats[\"pop\"]=stats.loc[:,[\"Likes\",\"Comments\",\"Views\"]].apply(lambda x:x[\"Likes\"]*0.3+x[\"Comments\"]*0.6+x['Views'],axis=1)\n",
    "stats.to_csv('data/pop_stats.csv')\n",
    "\n",
    "# pop 的值已经算出来，整合的数据集在目录 data/data/pop_stats.csv 下，可以用来做 时序分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 接下来是 **生成 tag** 和 为了做 **clustering** 的准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/qlchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/qlchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/qlchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/qlchen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Comment_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Let's not forget that Apple Pay in 2014 requir...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Let', \"'s\", 'forget', 'Apple', 'Pay', '2014'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Here in NZ 50% of retailers don’t even have co...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['NZ', '50', 'retailer', '’', 'even', 'contact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>I will forever acknowledge this channel with t...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>['forever', 'acknowledge', 'channel', 'help', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Whenever I go to a place that doesn’t take App...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Whenever', 'go', 'place', '’', 'take', 'Appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Apple Pay is so convenient, secure, and easy t...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>['Apple', 'Pay', 'convenient', 'secure', 'easy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>We’ve been hounding my bank to adopt Apple pay...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['’', 'hounding', 'bank', 'adopt', 'Apple', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>We only got Apple Pay in South Africa in 2020/...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>['got', 'Apple', 'Pay', 'South', 'Africa', '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>For now, I need both Apple Pay and the physica...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['need', 'Apple', 'Pay', 'physical', 'credit',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>In the United States, we have an abundance of ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>['United', 'States', 'abundance', 'retailer', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>In Cambodia, we have a universal QR code syste...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Cambodia', 'universal', 'QR', 'code', 'syste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Video ID                                            Comment  Likes  \\\n",
       "0  wAZZ-UWGVHI  Let's not forget that Apple Pay in 2014 requir...   95.0   \n",
       "1  wAZZ-UWGVHI  Here in NZ 50% of retailers don’t even have co...   19.0   \n",
       "2  wAZZ-UWGVHI  I will forever acknowledge this channel with t...  161.0   \n",
       "3  wAZZ-UWGVHI  Whenever I go to a place that doesn’t take App...    8.0   \n",
       "4  wAZZ-UWGVHI  Apple Pay is so convenient, secure, and easy t...   34.0   \n",
       "5  wAZZ-UWGVHI  We’ve been hounding my bank to adopt Apple pay...    8.0   \n",
       "6  wAZZ-UWGVHI  We only got Apple Pay in South Africa in 2020/...   29.0   \n",
       "7  wAZZ-UWGVHI  For now, I need both Apple Pay and the physica...    7.0   \n",
       "8  wAZZ-UWGVHI  In the United States, we have an abundance of ...    2.0   \n",
       "9  wAZZ-UWGVHI  In Cambodia, we have a universal QR code syste...   28.0   \n",
       "\n",
       "   Sentiment                                        Comment_tag  \n",
       "0        1.0  ['Let', \"'s\", 'forget', 'Apple', 'Pay', '2014'...  \n",
       "1        0.0  ['NZ', '50', 'retailer', '’', 'even', 'contact...  \n",
       "2        2.0  ['forever', 'acknowledge', 'channel', 'help', ...  \n",
       "3        0.0  ['Whenever', 'go', 'place', '’', 'take', 'Appl...  \n",
       "4        2.0  ['Apple', 'Pay', 'convenient', 'secure', 'easy...  \n",
       "5        1.0  ['’', 'hounding', 'bank', 'adopt', 'Apple', 'p...  \n",
       "6        2.0  ['got', 'Apple', 'Pay', 'South', 'Africa', '20...  \n",
       "7        1.0  ['need', 'Apple', 'Pay', 'physical', 'credit',...  \n",
       "8        2.0  ['United', 'States', 'abundance', 'retailer', ...  \n",
       "9        1.0  ['Cambodia', 'universal', 'QR', 'code', 'syste...  "
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词\n",
    "# 使用了 stopwords，词元化，标点，还有自定义的stopwords\n",
    "# 使用了 nltk 包里的 word_tokenize 对 sentence 进行 tokenization\n",
    "# 使用 WordNetLemmatizer 对 tokens 进行词元化分析和筛选\n",
    "\n",
    "# 生成有 tokens 列的表,如下的输出，但此时并没有通过 id 进行聚合，也还没提取出 关键词 ，因此这个表还不能用于 clustering\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# stopwords\n",
    "new_stopwords = [\"video\", \"s\", \"to\", \"on\", \"daily\"]\n",
    "stpwrd = nltk.corpus.stopwords.words('english')\n",
    "stpwrd.extend(new_stopwords)\n",
    "stpwrd.extend(string.punctuation)\n",
    "\n",
    "\n",
    "def generate_tag(comments):\n",
    "    word_tokens=word_tokenize(comments)\n",
    "    lemmatized_output =[wordnet_lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    filter_sen=[w for w in lemmatized_output if not w.lower() in stpwrd]\n",
    "\n",
    "    return filter_sen\n",
    "\n",
    "comment_words = ''\n",
    "\n",
    "\n",
    "comments['Comment_tag']=comments['Comment'].apply(generate_tag)\n",
    "\n",
    "comments['Comment_tag']=comments['Comment_tag'].apply(str)\n",
    "\n",
    "comments[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 根据 ID 聚合数据，这里保存的 csv 文件可以用来做 clustering，这是没经过 keyword 筛选的所有 tokens，当然也可以选择 第四点生成的 keywords 文件做，base on 你自己选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment_tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>--ZI0dSbbNU</th>\n",
       "      <td>['’', 'happy', 'eating', 'see', 'u', 'sufferin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--hxd1CrOqg</th>\n",
       "      <td>['’', 'really', 'unfortunate', 'intended', 'ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--ixiTypG8g</th>\n",
       "      <td>['biggest', 'problem', 'college', 'charging', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-64r1hcxtV4</th>\n",
       "      <td>['8:20-', '10:00', '.....', 'thought', 'happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-6IgkG5yZfo</th>\n",
       "      <td>['Hope', 'make', 'life', 'easier', '..', '00:4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-7hzaGya86g</th>\n",
       "      <td>['Yes', 'Dinner', 'dancing', 'Lotte', 'Mart', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8TnsjDRXUE</th>\n",
       "      <td>['ha', 'sponsored', 'Onnit', 'Visit', 'http', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9hjdvULDyc</th>\n",
       "      <td>['Hope', 'everyone', 'get', 'perfect', '1600',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-Cr69sGnrk8</th>\n",
       "      <td>['Let', \"'s\", 'appreciate', 'doe', 'far', \"'s\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-D4S6TpnO44</th>\n",
       "      <td>['💥', 'Get', 'Bonus', 'http', '//getbonus.link...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Comment_tag\n",
       "Video ID                                                      \n",
       "--ZI0dSbbNU  ['’', 'happy', 'eating', 'see', 'u', 'sufferin...\n",
       "--hxd1CrOqg  ['’', 'really', 'unfortunate', 'intended', 'ta...\n",
       "--ixiTypG8g  ['biggest', 'problem', 'college', 'charging', ...\n",
       "-64r1hcxtV4  ['8:20-', '10:00', '.....', 'thought', 'happen...\n",
       "-6IgkG5yZfo  ['Hope', 'make', 'life', 'easier', '..', '00:4...\n",
       "-7hzaGya86g  ['Yes', 'Dinner', 'dancing', 'Lotte', 'Mart', ...\n",
       "-8TnsjDRXUE  ['ha', 'sponsored', 'Onnit', 'Visit', 'http', ...\n",
       "-9hjdvULDyc  ['Hope', 'everyone', 'get', 'perfect', '1600',...\n",
       "-Cr69sGnrk8  ['Let', \"'s\", 'appreciate', 'doe', 'far', \"'s\"...\n",
       "-D4S6TpnO44  ['💥', 'Get', 'Bonus', 'http', '//getbonus.link..."
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=comments.groupby('Video ID')['Comment_tag'].apply(sum)\n",
    "data1.to_csv('data/comments_tag.csv')\n",
    "\n",
    "df_comment=pd.DataFrame(data1)\n",
    "\n",
    "df_comment[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 生成每个 video 的关键词 使用 summa 的 keywords 方法直接提取，生成的表，可以直接用于 clustering，这里生成的 keyword 才是代表每个 video 的 tag,数据保存在 data/comment_keyword_true_tag.csv 下，可以直接 read 做 clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords\n",
    "\n",
    "def generate_key(comm):\n",
    "\n",
    "    TR_keywords = keywords.keywords(comm, scores=True) \n",
    "    com_key=[]\n",
    "    for x,y in TR_keywords:\n",
    "        com_key.append(x)\n",
    "    return com_key\n",
    "\n",
    "\n",
    "\n",
    "df_comment['Comment_keyword']=df_comment['Comment_tag'].apply(generate_key)\n",
    "# for i range(len(df_comment)):\n",
    "#     df_comment['Comment'][i]=df_comment['Comment'][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Video ID\n",
       "--ZI0dSbbNU    [eating, eat, eats, fry, feel, smile, man, suf...\n",
       "--hxd1CrOqg    [russia, til, n, wo, realize, ukraine, analyza...\n",
       "--ixiTypG8g    [college, colleges, tuition, regulated, regula...\n",
       "-64r1hcxtV4    [life, inspiration, inspiring, inspire, health...\n",
       "-6IgkG5yZfo    [thankful, thank, thanks, time, help, helped, ...\n",
       "-7hzaGya86g    [good, like, mikey, dinner, food, eating, dump...\n",
       "-8TnsjDRXUE    [kid, amazing, know, knowing, guy, time, work,...\n",
       "-9hjdvULDyc    [reading, prediction, th, nd, wa, writing, tes...\n",
       "-Cr69sGnrk8    [doe, u, fan, thank, thanks, sccoby, support, ...\n",
       "-D4S6TpnO44                                           [getbonus]\n",
       "Name: Comment_keyword, dtype: object"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_comment.to_csv('data/comment_keyword_true_tag.csv')\n",
    "df_comment['Comment_keyword'][0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63b694122733cb65d0691538f646b747a4c54122ff439ca85f61f428093d1de8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
